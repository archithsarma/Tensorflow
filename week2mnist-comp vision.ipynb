import tensorflow as tf
from os import path, getcwd, chdir

#done on jupiter notebok web browser and hence this path....in a local environment get mnist.npz give the path to it instead 
 

path = f"{getcwd()}/../tmp2/mnist.npz"

def train_mnist():
    
    class myCallback(tf.keras.callbacks.Callback):
        def on_epoch_end(self, epoch, logs={}):
            if(logs.get('acc')>0.99):
              print("\nReached 99% accuracy so cancelling training!")
              self.model.stop_training = True
   

    mnist = tf.keras.datasets.mnist

    (x_train, y_train),(x_test, y_test) = mnist.load_data(path=path)
    
    x_train, x_test = x_train / 255.0, x_test / 255.0
    callbacks = myCallback()
    
    model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(512, activation=tf.nn.relu),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)
    ])

    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['acc'])
    
    
    history = model.fit(x_train, y_train, epochs=10, callbacks=[callbacks])
    return history.epoch, history.history['acc'][-1]
    train_mnist()




OUTPUT-----------------

Epoch 1/10
60000/60000 [==============================] - 13s 213us/sample - loss: 0.1999 - acc: 0.9409
Epoch 2/10
60000/60000 [==============================] - 12s 207us/sample - loss: 0.0807 - acc: 0.9754
Epoch 3/10
60000/60000 [==============================] - 12s 199us/sample - loss: 0.0528 - acc: 0.9834
Epoch 4/10
60000/60000 [==============================] - 12s 198us/sample - loss: 0.0365 - acc: 0.9883 - loss: 0.0369
Epoch 5/10
59904/60000 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9914
Reached 99% accuracy so cancelling training!
60000/60000 [==============================] - 12s 195us/sample - loss: 0.0271 - acc: 0.9913
([0, 1, 2, 3, 4], 0.9913333)
